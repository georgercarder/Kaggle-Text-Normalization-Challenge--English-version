Normalization is a collection of scripts that maps tokens to a configuration of characters determined by linuguistic, and contextual rules.
